{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import show\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "\n",
    "\n",
    "def plot_single_var(df, var):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    total = float(len(df)) # one person per row \n",
    "    #ax = sns.barplot(x=\"class\", hue=\"who\", data=titanic)\n",
    "    ax = sns.countplot(x=var, data=df, order = df[var].value_counts().index) \n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{0:.0%}'.format(height/total),\n",
    "                ha=\"center\") \n",
    "    show()\n",
    "    \n",
    "def plot_category_compare(var,group,title,df):\n",
    "    flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\",\"#708090\", \"#FFC0CB\",\"#C71585\", \n",
    "          \"#7B68EE\",'#4169E1','#6495ED',]\n",
    "    # mpl.style.use('seaborn')\n",
    "    # with sns.color_palette(\"husl\", 8):\n",
    "    #     ax = tb.plot(x = tb.index, kind='barh',stacked = True, title = title, mark_right = True)\n",
    "    tb = pd.crosstab(index=df[var],  columns=[ df[group]], normalize='index')\n",
    "    ax = tb.plot(x = tb.index, kind='barh',stacked = True, mark_right = True, color = flatui[:tb.shape[1]], title = title)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    total = float(len(tb)) # one person per row \n",
    "    size = tb.shape[0]\n",
    "    cnt = 0\n",
    "    for p in ax.patches:\n",
    "        height = 0\n",
    "        res =  cnt % size\n",
    "        ax.text(p.get_x()+p.get_width()/2.,height + res,\n",
    "                    '{0:.0%}'.format(p.get_width()),\n",
    "                    ha=\"center\")\n",
    "        cnt += 1\n",
    "    show()\n",
    "    \n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import scale, StandardScaler, Imputer, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "def cross_val(cols,model):\n",
    "    X_copy = X_train.reset_index()\n",
    "    y_copy = y_train.reset_index()\n",
    "    X_copy = X_copy.drop('index', axis = 1)\n",
    "    y_copy = y_copy.drop('index', axis = 1)\n",
    "    #df_y = np.where(df_ext['Performance'] == 'Good', 1,0)\n",
    "    kf = KFold(n_splits=5,random_state=1234, shuffle=True) # Define the split - into 10 folds \n",
    "    kf.get_n_splits(X_copy) # returns the number of splitting iterations in the cross-validator\n",
    "    \n",
    "    auc = []\n",
    "    pr_auc = []\n",
    "    for train_index, test_index in kf.split(X_copy):\n",
    "        X_tr, X_t = X_copy.loc[train_index, cols], X_copy.loc[test_index, cols]\n",
    "        y_tr, y_t = y_copy.iloc[train_index,:], y_copy.iloc[test_index,:]\n",
    "        model.fit(X_tr, y_tr.values.ravel()) \n",
    "        y_pred = model.predict_proba(X_t)[:,1]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_t,y_pred)\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_t,y_pred)\n",
    "        auc.append(metrics.auc(fpr, tpr))\n",
    "        pr_auc.append(metrics.auc(recall, precision, reorder=True))\n",
    "    return [np.mean(auc),np.mean(pr_auc)]\n",
    "\n",
    "def pred_model_res(X_train, y_train,X_test, y_test, model):\n",
    "    model = model.fit(X_train,y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test,y_pred)\n",
    "    return [metrics.auc(fpr, tpr), metrics.auc(recall, precision, reorder=True)]\n",
    "\n",
    "def search_model(x_train, y_train, est, param_grid, n_jobs = -1, cv = 5, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = GridSearchCV(estimator = est,\n",
    "                         param_grid = param_grid,\n",
    "                         scoring = 'roc_auc',\n",
    "                         verbose = 50,\n",
    "                         n_jobs = n_jobs,\n",
    "                         iid = True,\n",
    "                         refit = refit,\n",
    "                         cv = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def cross_val_select(X_train_trans, y_train_trans, model):\n",
    "    kf = KFold(n_splits=10,random_state=1234, shuffle=True) # Define the split - into 10 folds \n",
    "    kf.get_n_splits(X_train_trans) # returns the number of splitting iterations in the cross-validator\n",
    "    auc = []\n",
    "    pr_auc = []\n",
    "    for train_index, test_index in kf.split(X_train_trans):\n",
    "        X_tr, X_t = X_train_trans[train_index], X_train_trans[test_index]\n",
    "        y_tr, y_t = y_train.values[train_index], y_train.values[test_index]\n",
    "        model.fit(X_tr, y_tr) \n",
    "        y_pred = model.predict_proba(X_t)[:,1]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_t,y_pred)\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_t,y_pred)\n",
    "        auc.append(metrics.auc(fpr, tpr))\n",
    "        pr_auc.append(metrics.auc(recall, precision, reorder=True))\n",
    "    return [np.mean(auc),np.mean(pr_auc)]\n",
    "\n",
    "\n",
    "def eeo_analysis(X_val, y_pred, cols):\n",
    "    X_val_eeo = X_val.reset_index()\n",
    "    X_val_eeo = pd.concat([X_val_eeo,pd.DataFrame(y_pred,columns=['y_pred'])], axis =1)\n",
    "    mean = X_val_eeo.groupby('EEOC_CODE')['y_pred'].mean()\n",
    "    std = X_val_eeo.groupby('EEOC_CODE')['y_pred'].std()/np.sqrt(X_val_eeo['EEOC_CODE'].value_counts().sort_index())\n",
    "    plt.errorbar(mean.index, mean,xerr=0.5, yerr=2*std, linestyle='')\n",
    "    plt.show()\n",
    "    \n",
    "    race1 = X_val_eeo[X_val_eeo['EEOC_CODE'] == 1] \n",
    "    race2 = X_val_eeo[X_val_eeo['EEOC_CODE'] == 2] \n",
    "    # race 1\n",
    "    regr = linear_model.LinearRegression()\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(race1[cols], race1['y_pred'])\n",
    "    race1_coef = regr.coef_\n",
    "    race1_intercept = regr.intercept_ \n",
    "\n",
    "    # race 2\n",
    "    regr = linear_model.LinearRegression()\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(race2[cols], race2['y_pred'])\n",
    "    race2_coef = regr.coef_\n",
    "    race2_intercept = regr.intercept_ \n",
    "\n",
    "    avg_performance_race1 = race1['y_pred'].mean()\n",
    "    avg_performance_race2 = race2['y_pred'].mean()\n",
    "    tb = pd.DataFrame(((race1[cols].mean() - race2[cols].mean()) * race1_coef).sort_values(ascending = False), columns = ['diff'])\n",
    "    avg_diff = round(avg_performance_race1 - avg_performance_race2,4)\n",
    "    avg_diff = pd.DataFrame([avg_diff], columns = ['diff'])\n",
    "    tb = avg_diff.append(tb)\n",
    "    return tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train/ Test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['performance'], axis = 1) , df['performance'], \n",
    "                                                    test_size=0.2, random_state=1234, stratify = df['performance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression for diff cols\n",
    "auc_cv = []\n",
    "prauc_cv = []\n",
    "auc_test = []\n",
    "prauc_test = []\n",
    "for i in range(len(cols)):\n",
    "    log_model = linear_model.LogisticRegression(random_state = 1234, class_weight='balanced')\n",
    "    auc_cv_temp, prauc_cv_temp = cross_val(cols[i], log_model)\n",
    "    auc_cv.append(auc_cv_temp)\n",
    "    prauc_cv.append(prauc_cv_temp)\n",
    "    auc_test_temp, prauc_test_temp = pred_model_res(X_train[cols[i]], y_train, X_test[cols[i]], y_test,log_model)\n",
    "    auc_test.append(auc_test_temp)\n",
    "    prauc_test.append(prauc_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(prauc_cv)\n",
    "print(prauc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_axis = ['1.Orig Var','2.Log Var', '3.Min-Max Var','4.Categorical Var','5.Scale Var']\n",
    "plt.plot(x_axis,auc_cv)\n",
    "plt.plot(x_axis,auc_test)\n",
    "plt.legend(['CV AUC', 'Test AUC'])\n",
    "plt.title('Customer - K Selection for AUC Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K selection for cat\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "auc_cv_k = []\n",
    "prauc_cv_k = []\n",
    "auc_test_k = []\n",
    "prauc_test_k = []\n",
    "for i in range(30,X_train[cols[3]].shape[1] + 1):\n",
    "    log_model = linear_model.LogisticRegression(random_state = 1234, class_weight='balanced')\n",
    "    var_select = SelectKBest(score_func=chi2, k=i)\n",
    "    fit = var_select.fit(X_train[cols[3]], y_train)\n",
    "    X_train_trans = fit.transform(X_train[cols[3]])\n",
    "    X_test_trans = fit.transform(X_test[cols[3]])\n",
    "    auc_cv_temp, prauc_cv_temp = cross_val_select(X_train_trans, y_train, log_model)\n",
    "    auc_cv_k.append(auc_cv_temp)\n",
    "    prauc_cv_k.append(prauc_cv_temp)\n",
    "    auc_test_temp, prauc_test_temp = pred_model_res(X_train_trans, y_train, X_test_trans, y_test, log_model)\n",
    "    auc_test_k.append(auc_test_temp)\n",
    "    prauc_test_k.append(prauc_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x for x in range(30, len(cols[3])+1)],auc_cv_k)\n",
    "plt.plot([x for x in range(30, len(cols[3])+1)],auc_test_k)\n",
    "plt.legend(['CV AUC', 'Test AUC'])\n",
    "plt.title('customer - Categorized - K Selection for AUC Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = np.argmax(auc_cv_k) + 30 + 1\n",
    "best_k = featureScores.sort_values('Score', ascending=False).reset_index(drop=True)[:k]['Var'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_select = SelectKBest(score_func=chi2, k=k)\n",
    "fit = var_select.fit(X_train[cols[3]], y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train[cols[3]].columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Var','Score']  #naming the dataframe columns\n",
    "featureScores.to_csv('log_cat_K_select_79k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_model = linear_model.LogisticRegression(random_state = 1234, class_weight='balanced')\n",
    "log_model = log_model.fit(X_train[best_k],y_train)\n",
    "y_pred = log_model.predict_proba(X_test[best_k])[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "print('AUC: ', metrics.auc(fpr, tpr))\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test,y_pred)\n",
    "print('PR-AUC: ', metrics.auc(recall, precision, reorder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save log_cat model\n",
    "filename = 'logistic_model.pkl'\n",
    "pickle.dump(log_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "param_grid = {\n",
    "               #'min_samples_leaf' : [50,56,57,58,59,60,61,62,63,64,65] #[95,97,98,99,100,111,112,113,114,115]#\n",
    "               #'n_estimators':[10, 150, 200, 300, 500, 1000],\n",
    "               #'criterion':['gini', 'entropy'],\n",
    "               'max_depth': [1,2,3,4,5,6,7,8,9,10,15,20],\n",
    "               #'max_features': [\"auto\", \"sqrt\", \"log2\", None],\n",
    "               #'min_samples_split': [0.001,0.002,0.003,0.004]\n",
    "}\n",
    "model_rf = search_model(X_train[cols[3]], y_train, est = RandomForestClassifier(\n",
    "    n_estimators = 500, \n",
    "    #criterion = 'entropy',\n",
    "    #max_depth = 7, \n",
    "    #min_samples_leaf = 57,\n",
    "    #max_features = 'sqrt', \n",
    "    #min_samples_split = 0.003,\n",
    "    class_weight=\"balanced\"), param_grid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators = 500, \n",
    "     criterion = 'entropy',\n",
    "      max_depth = 5, \n",
    "      min_samples_leaf = 100,\n",
    "      max_features = None, \n",
    "      min_samples_split = 0.05,\n",
    "class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "auc_cv_temp, prauc_cv_temp = cross_val(cols[3], rf)\n",
    "auc_cv.append(auc_cv_temp)\n",
    "prauc_cv.append(prauc_cv_temp)\n",
    "auc_test_temp, prauc_test_temp = pred_model_res(X_train[cols[3]], y_train, X_test[cols[3]], y_test,rf)\n",
    "auc_test.append(auc_test_temp)\n",
    "prauc_test.append(prauc_test_temp)\n",
    "print(auc_cv_temp, prauc_cv_temp, auc_test_temp, prauc_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save model\n",
    "filename = 'randomforest_cat.pkl'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.81'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train[cols[3]], label=y_train.values.reshape(X_train[cols[3]].shape[0], 1))\n",
    "\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'binary:logistic'\n",
    "#     params['num_class'] = 3\n",
    "    params['eta'] = 0.01\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = True\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(X_train[cols[3]], label=y_train.values.reshape(X_train[cols[3]].shape[0], 1), missing=-1),\n",
    "                        num_boost_round=100000,\n",
    "                        nfold=5,\n",
    "                        metrics={'auc'},\n",
    "                        maximize=True,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        seed=1234,\n",
    "                        early_stopping_rounds=50)\n",
    "\n",
    "    return cv_results['test-auc-mean'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_BO = BayesianOptimization(xgb_evaluate,\n",
    "                              {'max_depth': (6, 20),#(5, 6, 7, 8, 9,10, 11,12,13,14, 15,20),#(6, 15),\n",
    "                               'min_child_weight': (0, 30),#( 1, 3, 5, 7,10,15,20,25,30,35,40,45,50 ),#(0, 10),\n",
    "                               'colsample_bytree': (0.1, 1),\n",
    "                               'subsample': (0.7, 1),\n",
    "                               'gamma': (0, 2)\n",
    "                               }\n",
    "                              )\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "xgb_BO_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model with bayes optimation\n",
    "import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 29.503411,\n",
    "    'subsample': 0.941049,\n",
    "    'colsample_bytree': 0.314091,\n",
    "    'gamma': 1.994352,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.01,\n",
    "    'seed': 1234}\n",
    "\n",
    "print(xgb_params)\n",
    "\n",
    "cv_results = xgb.cv(xgb_params, xgb.DMatrix(X_train[cols[3]], label=y_train),\n",
    "                    num_boost_round=1000000,\n",
    "                    nfold=5,\n",
    "                    maximize=True, \n",
    "                    stratified=True,\n",
    "                    shuffle=True,\n",
    "                    verbose_eval=20,\n",
    "                    seed=1234,\n",
    "                    early_stopping_rounds=100,\n",
    "                    metrics={'auc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_results['test-auc-mean']\n",
    "best_xgb_score = cv_results['test-auc-mean'].max()\n",
    "best_xgb_iteration = len(cv_results)\n",
    "\n",
    "print('best score:', best_xgb_score, 'best iterations:', best_xgb_iteration)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(xgb_params, \n",
    "                  xgb.DMatrix(X_train[cols[3]], label=y_train),\n",
    "                  num_boost_round=best_xgb_iteration\n",
    "                 )\n",
    "\n",
    "y_pred = model.predict(xgb.DMatrix(X_test[cols[3]]))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "print('AUC: ', metrics.auc(fpr, tpr))\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test,y_pred)\n",
    "print('PR-AUC: ', metrics.auc(recall, precision, reorder=True))\n",
    "df_cat = pd.concat([df_cat, pd.DataFrame(y_pred, columns=['xgb_cat']).reset_index(drop=True)],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_copy = X_train.reset_index()\n",
    "y_copy = y_train.reset_index()\n",
    "X_copy = X_copy.drop('index', axis = 1)\n",
    "y_copy = y_copy.drop('index', axis = 1)\n",
    "#df_y = np.where(df_ext['Performance'] == 'Good', 1,0)\n",
    "kf = KFold(n_splits=5,random_state=1234, shuffle=True) # Define the split - into 10 folds \n",
    "kf.get_n_splits(X_copy) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "auc = []\n",
    "pr_auc = []\n",
    "for train_index, test_index in kf.split(X_copy):\n",
    "    X_tr, X_t = X_copy.loc[train_index, cols[3]], X_copy.loc[test_index, cols[3]]\n",
    "    y_tr, y_t = y_copy.iloc[train_index,:], y_copy.iloc[test_index,:]\n",
    "    model = xgb.train(xgb_params, \n",
    "                  xgb.DMatrix(X_tr, label=y_tr.values.ravel()),\n",
    "                  num_boost_round=best_xgb_iteration\n",
    "                 )\n",
    "    #model.fit(X_tr, y_tr.values.ravel()) \n",
    "    #y_pred = model.predict_proba(X_t)[:,1]\n",
    "    y_pred = model.predict(xgb.DMatrix(X_t))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_t,y_pred)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_t,y_pred)\n",
    "    auc.append(metrics.auc(fpr, tpr))\n",
    "    pr_auc.append(metrics.auc(recall, precision, reorder=True))\n",
    "np.mean(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
